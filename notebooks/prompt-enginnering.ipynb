{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Enginnering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary libraries\n",
    "- datasets: For loading and managing datasets from Hugging Face\n",
    "- transformers: For accessing pre-trained language models and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the DialogSum dataset from Hugging Face\n",
    "This dataset contains dialogues paired with their summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select specific examples from the test set to demonstrate summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40,100]\n",
    "# Create a separator line for better output readability\n",
    "dash_line = \"-\".join('' for x in range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display the selected examples with their human-written summaries\n",
    "This helps establish a baseline for what good summaries look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(dash_line)\n",
    "    print(\"Input dialog:\")\n",
    "    print(dataset[\"test\"][index][\"dialogue\"])\n",
    "    print(dash_line)\n",
    "    print(\"Baseline human summary:\")\n",
    "    print(dataset[\"test\"][index][\"summary\"])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the pre-trained FLAN-T5 model and its associated tokenizer\n",
    "FLAN-T5 is a fine-tuned version of T5 with improved instruction following capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demonstrate basic tokenization process with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "print(f\"Original Sentence : {sentence}\")\n",
    "print(dash_line)\n",
    "\n",
    "# Convert the sentence to token IDs (encoding)\n",
    "sentence_encoded = tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(f\"Sentence Encoded : {sentence_encoded}\")\n",
    "print(dash_line)\n",
    "\n",
    "# Convert the token IDs back to text (decoding)\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(f\"Sentence Decoded : {sentence_decoded}\")\n",
    "print(dash_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. APPROACH 1: No-prompt summarization\n",
    "This approach feeds the dialogue directly to the model without any instructions.\n",
    "It relies on the model's pre-training to generate a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "    summary = dataset[\"test\"][index][\"summary\"]\n",
    "    print(dialogue)\n",
    "\n",
    "    # Tokenize the dialogue and generate a summary\n",
    "    inputs = tokenizer(dialogue, return_tensors=\"pt\")\n",
    "    model_output = model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0]\n",
    "    #print(model_output)\n",
    "    outputs = tokenizer.decode(model_output, skip_special_tokens=True)\n",
    "    print(f\"Model output - {outputs}\")\n",
    "    print(f\"Ground truth - {summary}\")\n",
    "    print(dash_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. APPROACH 2: Zero-shot prompting\n",
    "This approach includes an instruction to \"Summarize the following conversation\".\n",
    "Zero-shot means we don't provide any examples of summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "    summary = dataset[\"test\"][index][\"summary\"]\n",
    "    # Create a prompt that instructs the model to summarize the dialogue\n",
    "    prompt = f\"\"\"Summarize the following conversation:\n",
    "    {dialogue}\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "\n",
    "    # Tokenize the prompt and generate a summary\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    model_output = model.generate(inputs[\"input_ids\"], max_new_tokens=50, decoder_start_token_id=tokenizer.pad_token_id)[0]\n",
    "    #print(model_output)\n",
    "    outputs = tokenizer.decode(model_output, skip_special_tokens=True)\n",
    "    print(f\"Model output - {outputs}\")\n",
    "    print(f\"Ground truth - {summary}\")\n",
    "    print(dash_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. APPROACH 3: Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a prompt that includes examples of dialogues and their summaries before presenting the new dialogue to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    # Add example dialogues and their summaries to the prompt\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "        summary = dataset[\"test\"][index][\"summary\"]\n",
    "        prompt += f\"\"\"\\nDialogue:\n",
    "        {dialogue}\n",
    "\n",
    "        \\nWhat is going on?\\n\n",
    "        {summary}\n",
    "        \"\"\" \n",
    "\n",
    "    # Add the new dialogue to summarize (without its summary)\n",
    "    dialogue = dataset[\"test\"][example_index_to_summarize][\"dialogue\"]\n",
    "    prompt += f\"\"\"\\nDialogue:\n",
    "    {dialogue}\n",
    "    \\nWhat is going on?\\n\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the few-shot learning example\n",
    "# We use examples 40 and 100 as demonstrations, and summarize example 200\n",
    "example_indices_full = [40, 100]\n",
    "example_index_to_summarize = 200\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "#print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the ground truth summary for comparison\n",
    "summary = dataset[\"test\"][example_index_to_summarize][\"summary\"]\n",
    "\n",
    "# Generate a summary using the few-shot prompt with default generation parameters\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50, )[0], skip_special_tokens=True)\n",
    "print(f\"Ground truth - {summary}\")\n",
    "print(dash_line)\n",
    "print(f\"Model output - {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 4: Few-shot prompting with custom generation parameters\n",
    "Create a generation configuration with temperature control for more controlled output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_length=50, do_sample=True, temperature=0.1)\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                                         generation_config=generation_config)[0], \n",
    "                          skip_special_tokens=True)\n",
    "print(dash_line)\n",
    "print(f\"Model output with gen config- {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
